{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Derivatives"
      ],
      "metadata": {
        "id": "5Y9qkpkIEfAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import symbols, diff\n",
        "\n",
        "x = symbols('x')\n",
        "f = x**2 + 3*x + 2\n",
        "df = diff(f, x)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnRPFUMREh5J",
        "outputId": "26bc2ddf-9ff8-4b07-8e9e-3c56734d613a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2*x + 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical Derivative"
      ],
      "metadata": {
        "id": "ejQkeBRjFcxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def numerical_derivative(f, x, h=1e-5):\n",
        "  return (f(x + h) - f(x - h)) / (2 * h)\n",
        "\n",
        "def f(x):\n",
        "  return x**2 + 3*x + 2\n",
        "\n",
        "x_val = 1.0\n",
        "print(numerical_derivative(f, x_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDdR8NIRFbMI",
        "outputId": "24bb4422-198c-4697-be61-5dd733c5cfd0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.999999999988347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrals"
      ],
      "metadata": {
        "id": "t4KsVdIOGqvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sympy import symbols, integrate\n",
        "\n",
        "x = symbols('x')\n",
        "f = x**2 + 3*x + 2\n",
        "integral = integrate(f, x)\n",
        "print(integral)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maecF1w4Grrz",
        "outputId": "987f8277-2f07-4ab3-a06f-3f23378d0336"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x**3/3 + 3*x**2/2 + 2*x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical Integral"
      ],
      "metadata": {
        "id": "hwiWv2S_Hinn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.integrate import quad\n",
        "import numpy as np\n",
        "\n",
        "def f(x):\n",
        "  return x**2 + 3*x + 2\n",
        "\n",
        "result, error = quad(f, 0, 1)\n",
        "print(result, error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wu4WXkgHjfv",
        "outputId": "07e0ecb6-ccf7-4e8e-97e2-7c0bd952f85d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.833333333333333 4.2558549277297665e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent Implementation"
      ],
      "metadata": {
        "id": "Xi-DHEeUISiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(f, df, x0=0, lr=0.01, tol=1e-6, max_iter=1000):\n",
        "  x = x0\n",
        "  for i in range(max_iter):\n",
        "    grad = df(x)\n",
        "    if abs(grad) < tol:\n",
        "      break\n",
        "    x = x - lr * grad\n",
        "    print(f'Iteration {i+1}: x = {x:.6f}, f(x) = {f(x):.6f}, grad = {grad:.6f}')\n",
        "  return x\n",
        "\n",
        "def f(x):\n",
        "  return x**2\n",
        "\n",
        "def df(x):\n",
        "  return 2 * x\n",
        "\n",
        "min_x = gradient_descent(f, df, x0=10, lr=0.1)\n",
        "print(f'Minimum at x = {min_x:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxV6WxyIITbx",
        "outputId": "3bd6d187-3478-49b0-ef7c-400f62209d74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: x = 8.000000, f(x) = 64.000000, grad = 20.000000\n",
            "Iteration 2: x = 6.400000, f(x) = 40.960000, grad = 16.000000\n",
            "Iteration 3: x = 5.120000, f(x) = 26.214400, grad = 12.800000\n",
            "Iteration 4: x = 4.096000, f(x) = 16.777216, grad = 10.240000\n",
            "Iteration 5: x = 3.276800, f(x) = 10.737418, grad = 8.192000\n",
            "Iteration 6: x = 2.621440, f(x) = 6.871948, grad = 6.553600\n",
            "Iteration 7: x = 2.097152, f(x) = 4.398047, grad = 5.242880\n",
            "Iteration 8: x = 1.677722, f(x) = 2.814750, grad = 4.194304\n",
            "Iteration 9: x = 1.342177, f(x) = 1.801440, grad = 3.355443\n",
            "Iteration 10: x = 1.073742, f(x) = 1.152922, grad = 2.684355\n",
            "Iteration 11: x = 0.858993, f(x) = 0.737870, grad = 2.147484\n",
            "Iteration 12: x = 0.687195, f(x) = 0.472237, grad = 1.717987\n",
            "Iteration 13: x = 0.549756, f(x) = 0.302231, grad = 1.374390\n",
            "Iteration 14: x = 0.439805, f(x) = 0.193428, grad = 1.099512\n",
            "Iteration 15: x = 0.351844, f(x) = 0.123794, grad = 0.879609\n",
            "Iteration 16: x = 0.281475, f(x) = 0.079228, grad = 0.703687\n",
            "Iteration 17: x = 0.225180, f(x) = 0.050706, grad = 0.562950\n",
            "Iteration 18: x = 0.180144, f(x) = 0.032452, grad = 0.450360\n",
            "Iteration 19: x = 0.144115, f(x) = 0.020769, grad = 0.360288\n",
            "Iteration 20: x = 0.115292, f(x) = 0.013292, grad = 0.288230\n",
            "Iteration 21: x = 0.092234, f(x) = 0.008507, grad = 0.230584\n",
            "Iteration 22: x = 0.073787, f(x) = 0.005445, grad = 0.184467\n",
            "Iteration 23: x = 0.059030, f(x) = 0.003484, grad = 0.147574\n",
            "Iteration 24: x = 0.047224, f(x) = 0.002230, grad = 0.118059\n",
            "Iteration 25: x = 0.037779, f(x) = 0.001427, grad = 0.094447\n",
            "Iteration 26: x = 0.030223, f(x) = 0.000913, grad = 0.075558\n",
            "Iteration 27: x = 0.024179, f(x) = 0.000585, grad = 0.060446\n",
            "Iteration 28: x = 0.019343, f(x) = 0.000374, grad = 0.048357\n",
            "Iteration 29: x = 0.015474, f(x) = 0.000239, grad = 0.038686\n",
            "Iteration 30: x = 0.012379, f(x) = 0.000153, grad = 0.030949\n",
            "Iteration 31: x = 0.009904, f(x) = 0.000098, grad = 0.024759\n",
            "Iteration 32: x = 0.007923, f(x) = 0.000063, grad = 0.019807\n",
            "Iteration 33: x = 0.006338, f(x) = 0.000040, grad = 0.015846\n",
            "Iteration 34: x = 0.005071, f(x) = 0.000026, grad = 0.012677\n",
            "Iteration 35: x = 0.004056, f(x) = 0.000016, grad = 0.010141\n",
            "Iteration 36: x = 0.003245, f(x) = 0.000011, grad = 0.008113\n",
            "Iteration 37: x = 0.002596, f(x) = 0.000007, grad = 0.006490\n",
            "Iteration 38: x = 0.002077, f(x) = 0.000004, grad = 0.005192\n",
            "Iteration 39: x = 0.001662, f(x) = 0.000003, grad = 0.004154\n",
            "Iteration 40: x = 0.001329, f(x) = 0.000002, grad = 0.003323\n",
            "Iteration 41: x = 0.001063, f(x) = 0.000001, grad = 0.002658\n",
            "Iteration 42: x = 0.000851, f(x) = 0.000001, grad = 0.002127\n",
            "Iteration 43: x = 0.000681, f(x) = 0.000000, grad = 0.001701\n",
            "Iteration 44: x = 0.000544, f(x) = 0.000000, grad = 0.001361\n",
            "Iteration 45: x = 0.000436, f(x) = 0.000000, grad = 0.001089\n",
            "Iteration 46: x = 0.000348, f(x) = 0.000000, grad = 0.000871\n",
            "Iteration 47: x = 0.000279, f(x) = 0.000000, grad = 0.000697\n",
            "Iteration 48: x = 0.000223, f(x) = 0.000000, grad = 0.000558\n",
            "Iteration 49: x = 0.000178, f(x) = 0.000000, grad = 0.000446\n",
            "Iteration 50: x = 0.000143, f(x) = 0.000000, grad = 0.000357\n",
            "Iteration 51: x = 0.000114, f(x) = 0.000000, grad = 0.000285\n",
            "Iteration 52: x = 0.000091, f(x) = 0.000000, grad = 0.000228\n",
            "Iteration 53: x = 0.000073, f(x) = 0.000000, grad = 0.000183\n",
            "Iteration 54: x = 0.000058, f(x) = 0.000000, grad = 0.000146\n",
            "Iteration 55: x = 0.000047, f(x) = 0.000000, grad = 0.000117\n",
            "Iteration 56: x = 0.000037, f(x) = 0.000000, grad = 0.000094\n",
            "Iteration 57: x = 0.000030, f(x) = 0.000000, grad = 0.000075\n",
            "Iteration 58: x = 0.000024, f(x) = 0.000000, grad = 0.000060\n",
            "Iteration 59: x = 0.000019, f(x) = 0.000000, grad = 0.000048\n",
            "Iteration 60: x = 0.000015, f(x) = 0.000000, grad = 0.000038\n",
            "Iteration 61: x = 0.000012, f(x) = 0.000000, grad = 0.000031\n",
            "Iteration 62: x = 0.000010, f(x) = 0.000000, grad = 0.000025\n",
            "Iteration 63: x = 0.000008, f(x) = 0.000000, grad = 0.000020\n",
            "Iteration 64: x = 0.000006, f(x) = 0.000000, grad = 0.000016\n",
            "Iteration 65: x = 0.000005, f(x) = 0.000000, grad = 0.000013\n",
            "Iteration 66: x = 0.000004, f(x) = 0.000000, grad = 0.000010\n",
            "Iteration 67: x = 0.000003, f(x) = 0.000000, grad = 0.000008\n",
            "Iteration 68: x = 0.000003, f(x) = 0.000000, grad = 0.000006\n",
            "Iteration 69: x = 0.000002, f(x) = 0.000000, grad = 0.000005\n",
            "Iteration 70: x = 0.000002, f(x) = 0.000000, grad = 0.000004\n",
            "Iteration 71: x = 0.000001, f(x) = 0.000000, grad = 0.000003\n",
            "Iteration 72: x = 0.000001, f(x) = 0.000000, grad = 0.000003\n",
            "Iteration 73: x = 0.000001, f(x) = 0.000000, grad = 0.000002\n",
            "Iteration 74: x = 0.000001, f(x) = 0.000000, grad = 0.000002\n",
            "Iteration 75: x = 0.000001, f(x) = 0.000000, grad = 0.000001\n",
            "Iteration 76: x = 0.000000, f(x) = 0.000000, grad = 0.000001\n",
            "Minimum at x = 0.000000\n"
          ]
        }
      ]
    }
  ]
}